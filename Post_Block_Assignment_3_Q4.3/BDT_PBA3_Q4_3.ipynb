{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2NKav7tsYvM6vu/yaHPex",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Patric-Ramz/bdt-2023-26720051/blob/main/Post_Block_Assignment_3_Q4.3/BDT_PBA3_Q4_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8GDbGzOge56",
        "outputId": "7022ef49-cdfc-4f82-cca8-8e9e8ff44203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=12c8402455a66e8d6b6f345d1c469155ebda9d4f17a7bbb61b2043a5f86f08eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RandomForestExample\").getOrCreate()"
      ],
      "metadata": {
        "id": "_VsIgX9xhCml"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://storage.googleapis.com/bdt-spark-store/external_sources.csv -O gcs_external_sources.csv\n",
        "! wget https://storage.googleapis.com/bdt-spark-store/internal_data.csv -O gcs_internal_data.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JxiaRkahSHw",
        "outputId": "33e4ceea-b186-49a0-ce50-f1b2c52545ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-05 10:53:11--  https://storage.googleapis.com/bdt-spark-store/external_sources.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.193.207, 172.217.204.207, 172.217.203.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.193.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15503836 (15M) [text/csv]\n",
            "Saving to: ‘gcs_external_sources.csv’\n",
            "\n",
            "gcs_external_source 100%[===================>]  14.79M  15.2MB/s    in 1.0s    \n",
            "\n",
            "2023-11-05 10:53:12 (15.2 MB/s) - ‘gcs_external_sources.csv’ saved [15503836/15503836]\n",
            "\n",
            "--2023-11-05 10:53:13--  https://storage.googleapis.com/bdt-spark-store/internal_data.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.193.207, 172.217.204.207, 172.217.203.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.193.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 152978396 (146M) [text/csv]\n",
            "Saving to: ‘gcs_internal_data.csv’\n",
            "\n",
            "gcs_internal_data.c 100%[===================>] 145.89M  37.5MB/s    in 4.4s    \n",
            "\n",
            "2023-11-05 10:53:18 (33.3 MB/s) - ‘gcs_internal_data.csv’ saved [152978396/152978396]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.pandas as pd\n",
        "\n",
        "# Read in data from CSV files using the pandas API on Spark\n",
        "pdf_data = pd.read_csv('gcs_internal_data.csv')\n",
        "pdf_ext = pd.read_csv('gcs_external_sources.csv')\n",
        "\n",
        "# Join them on their common identifier key\n",
        "pdf_full = pdf_data.merge(pdf_ext, on='SK_ID_CURR')\n",
        "\n",
        "# Select columns\n",
        "columns_extract = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
        "                   'DAYS_BIRTH', 'DAYS_EMPLOYED', 'NAME_EDUCATION_TYPE',\n",
        "                   'DAYS_ID_PUBLISH', 'CODE_GENDER', 'AMT_ANNUITY',\n",
        "                   'DAYS_REGISTRATION', 'AMT_GOODS_PRICE', 'AMT_CREDIT',\n",
        "                   'ORGANIZATION_TYPE', 'DAYS_LAST_PHONE_CHANGE',\n",
        "                   'NAME_INCOME_TYPE', 'AMT_INCOME_TOTAL', 'OWN_CAR_AGE', 'TARGET']\n",
        "pdf = pdf_full[columns_extract]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6bLKCnSiI6Z",
        "outputId": "b03df93f-94ca-408c-cc7b-c27b23dd9b6c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, Imputer, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "df = pdf.to_spark()\n",
        "\n",
        "# Split the data into train and test sets\n",
        "(train, test) = df.randomSplit([0.8, 0.2], seed=101)\n",
        "\n",
        "# Handle categorical variables with StringIndexer and OneHotEncoder\n",
        "indexer = StringIndexer(inputCol=\"NAME_EDUCATION_TYPE\", outputCol=\"NAME_EDUCATION_TYPE_Index\")\n",
        "encoder = OneHotEncoder(inputCols=[\"NAME_EDUCATION_TYPE_Index\"], outputCols=[\"NAME_EDUCATION_TYPE_Vec\"])\n",
        "\n",
        "numeric_input_cols = [\n",
        "    'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
        "    'DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION',\n",
        "    'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'AMT_CREDIT',\n",
        "    'DAYS_LAST_PHONE_CHANGE', 'AMT_INCOME_TOTAL', 'OWN_CAR_AGE'\n",
        "]\n",
        "\n",
        "# Fill in missing data with Imputer\n",
        "imputer = Imputer(inputCols=numeric_input_cols, outputCols=numeric_input_cols, strategy=\"median\")\n",
        "\n",
        "# Initialize the VectorAssembler\n",
        "assembled_features = VectorAssembler(inputCols=numeric_input_cols, outputCol=\"features\")\n",
        "\n",
        "# Scale features with StandardScaler\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "\n",
        "# Define the Random Forest model\n",
        "rf = RandomForestClassifier(featuresCol=\"scaledFeatures\", labelCol=\"TARGET\", numTrees=100, seed=50)\n",
        "\n",
        "# Chain indexers and the forest in a Pipeline\n",
        "pipeline = Pipeline(stages=[indexer, encoder, imputer, assembled_features, scaler, rf])\n",
        "\n",
        "# Train the model\n",
        "model = pipeline.fit(train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Select example rows to display\n",
        "predictions.select(\"prediction\", \"TARGET\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"TARGET\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g\" % (1.0 - accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaKRkPugitMg",
        "outputId": "005d0010-c4d4-42d4-e632-bf3f09db8d48"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+\n",
            "|prediction|TARGET|\n",
            "+----------+------+\n",
            "|       0.0|     0|\n",
            "|       0.0|     0|\n",
            "|       0.0|     0|\n",
            "|       0.0|     0|\n",
            "|       0.0|     0|\n",
            "+----------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Test Error = 0.293573\n"
          ]
        }
      ]
    }
  ]
}